<!doctype html>
<html lang="en">
  
  <head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="stylesheet" href="static/css/main.css">
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
    integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  <script src="static/js/typeahead.bundle.js"></script>
  <link rel="stylesheet" href="static/css/typeahead.css">
  <link rel="stylesheet"
    href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
    crossorigin="anonymous">
  <link rel="stylesheet" href="static/css/lazy_load.css">
  <script src="static/js/lazy_load.js"></script>
  <title> ICLR: Neural Text Generation With Unlikelihood Training </title>
</head>
  <meta name="citation_title" content="Neural Text Generation With Unlikelihood Training" />
  
  <meta name="citation_author" content="Sean Welleck" />
  
  <meta name="citation_author" content="Ilia Kulikov" />
  
  <meta name="citation_author" content="Stephen Roller" />
  
  <meta name="citation_author" content="Emily Dinan" />
  
  <meta name="citation_author" content="Kyunghyun Cho" />
  
  <meta name="citation_author" content="Jason Weston" />
  
  <meta name="citation_publication_date" content="2020/04"/>
  <meta name="citation_conference_title" content="Eighth International Conference on Learning Representations" />
  <meta name="citation_abstract" content="Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques." />
  <meta name="citation_pdf_url" content="http://www.openreview.net/pdf?id=SJeYe0NtvH" />
  
  <meta name="citation_keywords" content="generation" />
  
  <meta name="citation_keywords" content="language modeling" />
  
  <meta name="citation_keywords" content="nlp" />
  
  <meta name="citation_keywords" content="text generation" />
  
  <body >
    <!-- NAV -->
    
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.24.0/moment.min.js"
  integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment-timezone/0.5.28/moment-timezone-with-data.min.js"
  integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
  crossorigin="anonymous"></script>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="index.html">
    <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          
          <a class="nav-link" href="index.html">Home</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="calendar.html">Schedule</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="workshops.html">Workshops</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="papers.html">Papers</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="about.html">Help</a>
          
        </li>
        
      </ul>
    </div>
  </div>
</nav>
    <div class="container">
      <!-- Title -->
      <div class="pp-card m-3" style="">
        <div class="card-header">
          <h2 class="card-title main-title text-center" style="">
            Neural Text Generation With Unlikelihood Training
          </h2>
          <h3 class="card-subtitle mb-2 text-muted text-center">
            
            <a href="papers.html?filter=authors&search=Sean Welleck" class="text-muted">Sean Welleck</a>,
            
            <a href="papers.html?filter=authors&search=Ilia Kulikov" class="text-muted">Ilia Kulikov</a>,
            
            <a href="papers.html?filter=authors&search=Stephen Roller" class="text-muted">Stephen Roller</a>,
            
            <a href="papers.html?filter=authors&search=Emily Dinan" class="text-muted">Emily Dinan</a>,
            
            <a href="papers.html?filter=authors&search=Kyunghyun Cho" class="text-muted">Kyunghyun Cho</a>,
            
            <a href="papers.html?filter=authors&search=Jason Weston" class="text-muted">Jason Weston</a>
            
          </h3>
          <p class="card-text text-center"><span class="">Keywords:</span>
            
            <a href="papers.html?filter=keywords&search=generation" class="text-secondary text-decoration-none">generation</a>,
            
            <a href="papers.html?filter=keywords&search=language modeling" class="text-secondary text-decoration-none">language modeling</a>,
            
            <a href="papers.html?filter=keywords&search=nlp" class="text-secondary text-decoration-none">nlp</a>,
            
            <a href="papers.html?filter=keywords&search=text generation" class="text-secondary text-decoration-none">text generation</a>
            
          </p>
          <div class="text-center p-3">
            <a class="card-link" data-toggle="collapse" role="button" href="#details">
            Abstract
            </a>
            <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf?id=SJeYe0NtvH">
            Paper
            </a>
            
            <a href="https://drive.google.com/open?id=1rTksP8hubiXcYzJ8RBl83R8Ent5EtLOj" target="_blank"  class="card-link">
            Code
            </a>
            
            <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=SJeYe0NtvH">
            Reviews
            </a>
            <!--  -->
            <!-- <a href="38925733" target="_blank"  class="card-link"> -->
            <!--   Slides -->
            <!-- </a> -->
            <!--  -->
            <!-- </div> -->
            <!-- <div class="text-center "> -->
            <!-- <a href="chat.html?room=channel/poster_385" target="_blank"  class="card-link"> -->
            <!-- Chat -->
            <!-- </a> -->
          </div>
          <div class=" text-center text-muted text-monospace ">
            
          </div>
        </div>
      </div>
      <div id="details" class="pp-card m-3 collapse">
        <div class="card-body">
          <p class="card-text">
          <div id="abstractExample">
            <span class="font-weight-bold">Abstract:</span>
            Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.
          </div>
          </p>
          <p></p>
        </div>
      </div>
    </div>
    <!-- SlidesLive -->
    <div class="container" style="background-color:white; padding: 0px;">
      <div class="row m-2">
        <div class="col-md-12 col-xs-12 my-auto p-2" >

          <div id="presentation-embed-38925733" class="slp my-auto"></div>
          <script src='https://slideslive.com/embed_presentation.js'></script>
          <script>
            embed = new SlidesLiveEmbed('presentation-embed-38925733', {
            presentationId: '38925733',
            autoPlay: false, // change to true to autoplay the embedded presentation
            verticalEnabled: true,
            allowHiddenControlsWhenPaused: true,
            hideTitle: true
            });
          </script>
        </div>
      </div>
    </div>

    <!-- Recs -->
    <p></p>
    <div  class="container" style="padding-bottom: 30px; padding-top:30px">
      <center>
        <h2> Similar Papers </h2>
      </center>
    </div>
    <p></p>
    <div  class="container" >
      <div class="row">
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_rygGQyrFvH.html" class="text-muted">
                <h5 class="card-title" align="center">The Curious Case of Neural Text Degeneration</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                Ari Holtzman,
                
                Jan Buys,
                
                Li Du,
                
                Maxwell Forbes,
                
                Yejin Choi,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rygGQyrFvH.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_B1l4SgHKDH.html" class="text-muted">
                <h5 class="card-title" align="center">Residual Energy-Based Models for Text Generation</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                Yuntian Deng,
                
                Anton Bakhtin,
                
                Myle Ott,
                
                Arthur Szlam,
                
                Marc&#39;Aurelio Ranzato,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/B1l4SgHKDH.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_S1efxTVYDr.html" class="text-muted">
                <h5 class="card-title" align="center">Data-dependent Gaussian Prior Objective for Language Generation</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                Zuchao Li,
                
                Rui Wang,
                
                Kehai Chen,
                
                Masso Utiyama,
                
                Eiichiro Sumita,
                
                Zhuosheng Zhang,
                
                Hai Zhao,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/S1efxTVYDr.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
      </div>
    </div>
    <script type="text/javascript">
  $(document).ready(function () {
  
  if (location.hash !== '') {
  $('a[href="' + location.hash + '"]').tab('show');
  }
  
  $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
  var hash = $(e.target).attr("href");
  if (hash.substr(0,1) == "#") {
  var position = $(window).scrollTop();
  location.replace("#" + hash.substr(1));
  $(window).scrollTop(position);
  }
  });
  
  });
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163955028-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  
  gtag('config', 'UA-163955028-1');
</script>
<footer class="footer bg-light p-4">
  <div class="container">
    <p class="float-right"><a href="#">Back to Top</a></p>
    <p class="text-center">Â© 2020 International Conference on Learning Representations</p>
  </div>
</footer>
  </body>
  <script src="static/js/time-extend.js"></script>
<script>
  $(document).ready(()=>{
    add_local_tz('.session_times');
  })
</script>
</html>