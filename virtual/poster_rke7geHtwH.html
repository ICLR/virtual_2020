<!doctype html>
<html lang="en">
  
  <head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="stylesheet" href="static/css/main.css">
  <script src="https://d3js.org/d3.v5.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
    integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  <script src="static/js/typeahead.bundle.js"></script>
  <link rel="stylesheet" href="static/css/typeahead.css">
  <link rel="stylesheet"
    href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
    crossorigin="anonymous">
  <link rel="stylesheet" href="static/css/lazy_load.css">
  <script src="static/js/lazy_load.js"></script>
  <title> ICLR: Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning </title>
</head>
  <meta name="citation_title" content="Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning" />
  
  <meta name="citation_author" content="Noah Siegel" />
  
  <meta name="citation_author" content="Jost Tobias Springenberg" />
  
  <meta name="citation_author" content="Felix Berkenkamp" />
  
  <meta name="citation_author" content="Abbas Abdolmaleki" />
  
  <meta name="citation_author" content="Michael Neunert" />
  
  <meta name="citation_author" content="Thomas Lampe" />
  
  <meta name="citation_author" content="Roland Hafner" />
  
  <meta name="citation_author" content="Nicolas Heess" />
  
  <meta name="citation_author" content="Martin Riedmiller" />
  
  <meta name="citation_publication_date" content="2020/04"/>
  <meta name="citation_conference_title" content="Eighth International Conference on Learning Representations" />
  <meta name="citation_abstract" content="Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. Our method can be seen as an extension of recent work on batch-RL that enables stable learning from conflicting data-sources. We find  improvements on competitive baselines in a variety of RL tasks -- including standard continuous control benchmarks and multi-task learning for simulated and real-world robots. " />
  <meta name="citation_pdf_url" content="http://www.openreview.net/pdf?id=rke7geHtwH" />
  
  <meta name="citation_keywords" content="continuous control" />
  
  <meta name="citation_keywords" content="off policy" />
  
  <meta name="citation_keywords" content="reinforcement learning" />
  
  <body >
    <!-- NAV -->
    
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.24.0/moment.min.js"
  integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment-timezone/0.5.28/moment-timezone-with-data.min.js"
  integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww="
  crossorigin="anonymous"></script>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="index.html">
    <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          
          <a class="nav-link" href="index.html">Home</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="calendar.html">Schedule</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="workshops.html">Workshops</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="papers.html">Papers</a>
          
        </li>
        
        <li class="nav-item ">
          
          <a class="nav-link" href="about.html">Help</a>
          
        </li>
        
      </ul>
    </div>
  </div>
</nav>
    <div class="container">
      <!-- Title -->
      <div class="pp-card m-3" style="">
        <div class="card-header">
          <h2 class="card-title main-title text-center" style="">
            Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning
          </h2>
          <h3 class="card-subtitle mb-2 text-muted text-center">
            
            <a href="papers.html?filter=authors&search=Noah Siegel" class="text-muted">Noah Siegel</a>,
            
            <a href="papers.html?filter=authors&search=Jost Tobias Springenberg" class="text-muted">Jost Tobias Springenberg</a>,
            
            <a href="papers.html?filter=authors&search=Felix Berkenkamp" class="text-muted">Felix Berkenkamp</a>,
            
            <a href="papers.html?filter=authors&search=Abbas Abdolmaleki" class="text-muted">Abbas Abdolmaleki</a>,
            
            <a href="papers.html?filter=authors&search=Michael Neunert" class="text-muted">Michael Neunert</a>,
            
            <a href="papers.html?filter=authors&search=Thomas Lampe" class="text-muted">Thomas Lampe</a>,
            
            <a href="papers.html?filter=authors&search=Roland Hafner" class="text-muted">Roland Hafner</a>,
            
            <a href="papers.html?filter=authors&search=Nicolas Heess" class="text-muted">Nicolas Heess</a>,
            
            <a href="papers.html?filter=authors&search=Martin Riedmiller" class="text-muted">Martin Riedmiller</a>
            
          </h3>
          <p class="card-text text-center"><span class="">Keywords:</span>
            
            <a href="papers.html?filter=keywords&search=continuous control" class="text-secondary text-decoration-none">continuous control</a>,
            
            <a href="papers.html?filter=keywords&search=off policy" class="text-secondary text-decoration-none">off policy</a>,
            
            <a href="papers.html?filter=keywords&search=reinforcement learning" class="text-secondary text-decoration-none">reinforcement learning</a>
            
          </p>
          <div class="text-center p-3">
            <a class="card-link" data-toggle="collapse" role="button" href="#details">
            Abstract
            </a>
            <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf?id=rke7geHtwH">
            Paper
            </a>
            
            <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=rke7geHtwH">
            Reviews
            </a>
            <!--  -->
            <!-- <a href="38926063" target="_blank"  class="card-link"> -->
            <!--   Slides -->
            <!-- </a> -->
            <!--  -->
            <!-- </div> -->
            <!-- <div class="text-center "> -->
            <!-- <a href="chat.html?room=channel/poster_582" target="_blank"  class="card-link"> -->
            <!-- Chat -->
            <!-- </a> -->
          </div>
          <div class=" text-center text-muted text-monospace ">
            
          </div>
        </div>
      </div>
      <div id="details" class="pp-card m-3 collapse">
        <div class="card-body">
          <p class="card-text">
          <div id="abstractExample">
            <span class="font-weight-bold">Abstract:</span>
            Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. Our method can be seen as an extension of recent work on batch-RL that enables stable learning from conflicting data-sources. We find  improvements on competitive baselines in a variety of RL tasks -- including standard continuous control benchmarks and multi-task learning for simulated and real-world robots. 
          </div>
          </p>
          <p></p>
        </div>
      </div>
    </div>
    <!-- SlidesLive -->
    <div class="container" style="background-color:white; padding: 0px;">
      <div class="row m-2">
        <div class="col-md-12 col-xs-12 my-auto p-2" >

          <div id="presentation-embed-38926063" class="slp my-auto"></div>
          <script src='https://slideslive.com/embed_presentation.js'></script>
          <script>
            embed = new SlidesLiveEmbed('presentation-embed-38926063', {
            presentationId: '38926063',
            autoPlay: false, // change to true to autoplay the embedded presentation
            verticalEnabled: true,
            allowHiddenControlsWhenPaused: true,
            hideTitle: true
            });
          </script>
        </div>
      </div>
    </div>

    <!-- Recs -->
    <p></p>
    <div  class="container" style="padding-bottom: 30px; padding-top:30px">
      <center>
        <h2> Similar Papers </h2>
      </center>
    </div>
    <p></p>
    <div  class="container" >
      <div class="row">
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_SylOlp4FvH.html" class="text-muted">
                <h5 class="card-title" align="center">V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                H. Francis Song,
                
                Abbas Abdolmaleki,
                
                Jost Tobias Springenberg,
                
                Aidan Clark,
                
                Hubert Soyer,
                
                Jack W. Rae,
                
                Seb Noury,
                
                Arun Ahuja,
                
                Siqi Liu,
                
                Dhruva Tirumala,
                
                Nicolas Heess,
                
                Dan Belov,
                
                Martin Riedmiller,
                
                Matthew M. Botvinick,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/SylOlp4FvH.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_S1xKd24twB.html" class="text-muted">
                <h5 class="card-title" align="center">SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                Siddharth Reddy,
                
                Anca D. Dragan,
                
                Sergey Levine,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/S1xKd24twB.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
        <div class="col-md-4 col-xs-6">
          <div class="pp-card" >
            <div class="pp-card-header" class="text-muted">
              <a href="poster_HJgC60EtwB.html" class="text-muted">
                <h5 class="card-title" align="center">Robust Reinforcement Learning for Continuous Control with Model Misspecification</h5>
              </a>
              <h6 class="card-subtitle text-muted" align="center">
                
                Daniel J. Mankowitz,
                
                Nir Levine,
                
                Rae Jeong,
                
                Abbas Abdolmaleki,
                
                Jost Tobias Springenberg,
                
                Yuanyuan Shi,
                
                Jackie Kay,
                
                Todd Hester,
                
                Timothy Mann,
                
                Martin Riedmiller,
                
              </h6>
              <center><img class="cards_img" src="https://iclr.github.io/iclr-images/HJgC60EtwB.png" width="80%"/></center>
            </div>
          </div>
        </div>
        
      </div>
    </div>
    <script type="text/javascript">
  $(document).ready(function () {
  
  if (location.hash !== '') {
  $('a[href="' + location.hash + '"]').tab('show');
  }
  
  $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
  var hash = $(e.target).attr("href");
  if (hash.substr(0,1) == "#") {
  var position = $(window).scrollTop();
  location.replace("#" + hash.substr(1));
  $(window).scrollTop(position);
  }
  });
  
  });
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163955028-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  
  gtag('config', 'UA-163955028-1');
</script>
<footer class="footer bg-light p-4">
  <div class="container">
    <p class="float-right"><a href="#">Back to Top</a></p>
    <p class="text-center">Â© 2020 International Conference on Learning Representations</p>
  </div>
</footer>
  </body>
  <script src="static/js/time-extend.js"></script>
<script>
  $(document).ready(()=>{
    add_local_tz('.session_times');
  })
</script>
</html>